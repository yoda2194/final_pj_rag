{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master\n",
      "  Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to /tmp/pip-req-build-th8f9po0\n",
      "  Running command git clone --filter=blob:none --quiet 'https://****@github.com/SKTBrain/KoBERT.git' /tmp/pip-req-build-th8f9po0\n",
      "  Resolved https://****@github.com/SKTBrain/KoBERT.git to commit 5c46b1c68e4755b54879431bd302db621f4d2f47\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: boto3<=1.15.18 in /home/ryud904/miniconda3/envs/kobert/lib/python3.7/site-packages (from kobert==0.2.3) (1.15.18)\n",
      "Requirement already satisfied: gluonnlp<=0.10.0,>=0.6.0 in /home/ryud904/miniconda3/envs/kobert/lib/python3.7/site-packages (from kobert==0.2.3) (0.10.0)\n",
      "Requirement already satisfied: mxnet<=1.7.0.post2,>=1.4.0 in /home/ryud904/miniconda3/envs/kobert/lib/python3.7/site-packages (from kobert==0.2.3) (1.7.0.post2)\n",
      "Requirement already satisfied: onnxruntime<=1.8.0,==1.8.0 in /home/ryud904/miniconda3/envs/kobert/lib/python3.7/site-packages (from kobert==0.2.3) (1.8.0)\n",
      "Requirement already satisfied: sentencepiece<=0.1.96,>=0.1.6 in /home/ryud904/miniconda3/envs/kobert/lib/python3.7/site-packages (from kobert==0.2.3) (0.1.96)\n",
      "Requirement already satisfied: torch<=1.10.1,>=1.7.0 in /home/ryud904/miniconda3/envs/kobert/lib/python3.7/site-packages (from kobert==0.2.3) (1.10.1)\n",
      "Requirement already satisfied: transformers<=4.8.1,>=4.8.1 in /home/ryud904/miniconda3/envs/kobert/lib/python3.7/site-packages (from kobert==0.2.3) (4.8.1)\n",
      "Requirement already satisfied: flatbuffers in /home/ryud904/miniconda3/envs/kobert/lib/python3.7/site-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (24.12.23)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /home/ryud904/miniconda3/envs/kobert/lib/python3.7/site-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (1.21.6)\n",
      "Requirement already satisfied: protobuf in /home/ryud904/miniconda3/envs/kobert/lib/python3.7/site-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (4.24.4)\n",
      "Requirement already satisfied: botocore<1.19.0,>=1.18.18 in /home/ryud904/miniconda3/envs/kobert/lib/python3.7/site-packages (from boto3<=1.15.18->kobert==0.2.3) (1.18.18)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ryud904/miniconda3/envs/kobert/lib/python3.7/site-packages (from boto3<=1.15.18->kobert==0.2.3) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ryud904/miniconda3/envs/kobert/lib/python3.7/site-packages (from boto3<=1.15.18->kobert==0.2.3) (0.3.7)\n",
      "Requirement already satisfied: cython in /home/ryud904/miniconda3/envs/kobert/lib/python3.7/site-packages (from gluonnlp<=0.10.0,>=0.6.0->kobert==0.2.3) (3.0.11)\n",
      "Requirement already satisfied: packaging in /home/ryud904/miniconda3/envs/kobert/lib/python3.7/site-packages (from gluonnlp<=0.10.0,>=0.6.0->kobert==0.2.3) (24.0)\n",
      "Requirement already satisfied: requests<3,>=2.20.0 in /home/ryud904/miniconda3/envs/kobert/lib/python3.7/site-packages (from mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (2.31.0)\n",
      "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /home/ryud904/miniconda3/envs/kobert/lib/python3.7/site-packages (from mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (0.8.4)\n",
      "Requirement already satisfied: typing_extensions in /home/ryud904/miniconda3/envs/kobert/lib/python3.7/site-packages (from torch<=1.10.1,>=1.7.0->kobert==0.2.3) (4.7.1)\n",
      "Requirement already satisfied: sacremoses in /home/ryud904/miniconda3/envs/kobert/lib/python3.7/site-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (0.0.53)\n",
      "Requirement already satisfied: pyyaml in /home/ryud904/miniconda3/envs/kobert/lib/python3.7/site-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (6.0.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ryud904/miniconda3/envs/kobert/lib/python3.7/site-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (4.67.1)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/ryud904/miniconda3/envs/kobert/lib/python3.7/site-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (0.10.3)\n",
      "Requirement already satisfied: huggingface-hub==0.0.12 in /home/ryud904/miniconda3/envs/kobert/lib/python3.7/site-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (0.0.12)\n",
      "Requirement already satisfied: filelock in /home/ryud904/miniconda3/envs/kobert/lib/python3.7/site-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (3.12.2)\n",
      "Requirement already satisfied: importlib-metadata in /home/ryud904/miniconda3/envs/kobert/lib/python3.7/site-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (6.7.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ryud904/miniconda3/envs/kobert/lib/python3.7/site-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (2024.4.16)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ryud904/miniconda3/envs/kobert/lib/python3.7/site-packages (from botocore<1.19.0,>=1.18.18->boto3<=1.15.18->kobert==0.2.3) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3<1.26,>=1.20 in /home/ryud904/miniconda3/envs/kobert/lib/python3.7/site-packages (from botocore<1.19.0,>=1.18.18->boto3<=1.15.18->kobert==0.2.3) (1.25.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ryud904/miniconda3/envs/kobert/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (3.4.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ryud904/miniconda3/envs/kobert/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (2024.8.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ryud904/miniconda3/envs/kobert/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (3.10)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ryud904/miniconda3/envs/kobert/lib/python3.7/site-packages (from importlib-metadata->transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (3.15.0)\n",
      "Requirement already satisfied: six in /home/ryud904/miniconda3/envs/kobert/lib/python3.7/site-packages (from sacremoses->transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (1.17.0)\n",
      "Requirement already satisfied: joblib in /home/ryud904/miniconda3/envs/kobert/lib/python3.7/site-packages (from sacremoses->transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (1.3.2)\n",
      "Requirement already satisfied: click in /home/ryud904/miniconda3/envs/kobert/lib/python3.7/site-packages (from sacremoses->transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (8.1.8)\n",
      "Building wheels for collected packages: kobert\n",
      "  Building wheel for kobert (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kobert: filename=kobert-0.2.3-py3-none-any.whl size=15695 sha256=287bdc557b8cf83bef970a2e3d381d6c1a0daa9a61987b0cc60908fc59784434\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-fvmgl3pm/wheels/d3/68/ca/334747dfb038313b49cf71f84832a33372f3470d9ddfd051c0\n",
      "Successfully built kobert\n",
      "Installing collected packages: kobert\n",
      "Successfully installed kobert-0.2.3\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kobert\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "\n",
    "#transformers\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPU 사용\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model. /mnt/c/Users/Admin/Documents/kobert_project/.cache/kobert_v1.zip\n",
      "using cached model. /mnt/c/Users/Admin/Documents/kobert_project/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
     ]
    }
   ],
   "source": [
    "#BERT 모델, Vocabulary 불러오기\n",
    "bertmodel, vocab = get_pytorch_kobert_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data 불러오기<br>\n",
    "label에 대한 고민 <br>\n",
    "<b>dataset 1(한국어 감정 정보가 포함된 단발성 대화 데이터셋)</b> : 공포 놀람 분노 슬픔 중립 행복 혐오 <br>\n",
    "<b>dataset 2(감성 대화 말뭉치)</b> : 분노 슬픔 당황 기쁨 불안 상처 <br>\n",
    "<br>\n",
    "<b>5가지 분류</b> -> 분노혐오 / 중립 (행복 포함) / 슬픔 / 놀람 당황 / 공포 불안 <br>\n",
    "<b>3가지 분류</b> -> 분노혐오 / 중립 (행복, 슬픔, 놀람, 당황) / 걱정(공포, 불안)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentence          0\n",
       "Emotion           0\n",
       "Unnamed: 2    38594\n",
       "Unnamed: 3    38594\n",
       "Unnamed: 4    38594\n",
       "공포            38587\n",
       "5468.0        38587\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "dataset = pd.read_excel(\"/mnt/c/Users/Admin/documents/kobert_project/한국어_단발성_대화_데이터셋.xlsx\")\n",
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>언니 동생으로 부르는게 맞는 일인가요..??</td>\n",
       "      <td>공포</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>그냥 내 느낌일뿐겠지?</td>\n",
       "      <td>공포</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>아직너무초기라서 그런거죠?</td>\n",
       "      <td>공포</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>유치원버스 사고 낫다던데</td>\n",
       "      <td>공포</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>근데 원래이런거맞나요</td>\n",
       "      <td>공포</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Sentence Emotion\n",
       "0  언니 동생으로 부르는게 맞는 일인가요..??      공포\n",
       "1              그냥 내 느낌일뿐겠지?      공포\n",
       "2            아직너무초기라서 그런거죠?      공포\n",
       "3             유치원버스 사고 낫다던데      공포\n",
       "4               근데 원래이런거맞나요      공포"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.dropna(axis=1, inplace=True)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11346          트럼프 70대 부인 40대.. 딸같은 ..\n",
       "11347                      말이 많았을건데???\n",
       "11348                         홍체인식????\n",
       "11349                의외로 기능도 어려워 하시던데.\n",
       "11350     세금 제대로 내는 직업에 종사하는 경우 드물다...\n",
       "11351      저 여자애도 대단하지만 저 남자애가 더 대단하다.\n",
       "11352                  우와 박보거어어어엄!!!!!\n",
       "11353                 스마트폰이 앱을 사용치말라고?\n",
       "11354            자사 제품 쓰지말라 한거면 말다했네..\n",
       "11355                  설마 진짜 믿는건 아니겠지?\n",
       "11356        스위스 정부의 법안이 아니야 ㅋㅋㅋㅋ ㅋㅋㅋㅋ\n",
       "11357                          아이 큰일났네\n",
       "11358           그만큼 수비 집중하게 햇던 아이버슨이라니\n",
       "11359                 걷기 단추 눌렀더니 폭발...\n",
       "11360                   더더군다나 그게 약소국..\n",
       "11361                시대가 어느시대인데 사람이 뽑냐\n",
       "11362                   진심 내 인생 드라마임..\n",
       "11363                       대단하다... 진짜\n",
       "11364                   헐 김종민이가 대상급인가?\n",
       "11365                    7로 나온다든데 아니었나\n",
       "Name: Sentence, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.loc[dataset.Emotion == '놀람','Sentence'].tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "행복    6037\n",
       "놀람    5898\n",
       "분노    5665\n",
       "공포    5468\n",
       "혐오    5429\n",
       "슬픔    5267\n",
       "중립    4830\n",
       "Name: Emotion, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['Emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "행복    6037\n",
       "분노    5665\n",
       "공포    5468\n",
       "혐오    5429\n",
       "중립    4830\n",
       "Name: Emotion, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.loc[dataset.Emotion != '놀람', :]\n",
    "dataset = dataset.loc[dataset.Emotion != '슬픔', :]\n",
    "dataset['Emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>공포</th>\n",
       "      <th>5468.0</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>언니 동생으로 부르는게 맞는 일인가요..??</td>\n",
       "      <td>공포</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>놀람</td>\n",
       "      <td>5898.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>그냥 내 느낌일뿐겠지?</td>\n",
       "      <td>공포</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>분노</td>\n",
       "      <td>5665.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>아직너무초기라서 그런거죠?</td>\n",
       "      <td>공포</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>슬픔</td>\n",
       "      <td>5267.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>유치원버스 사고 낫다던데</td>\n",
       "      <td>공포</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>중립</td>\n",
       "      <td>4830.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>근데 원래이런거맞나요</td>\n",
       "      <td>공포</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>행복</td>\n",
       "      <td>6037.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Sentence Emotion  Unnamed: 2  Unnamed: 3  Unnamed: 4  공포  \\\n",
       "0  언니 동생으로 부르는게 맞는 일인가요..??      공포         NaN         NaN         NaN  놀람   \n",
       "1              그냥 내 느낌일뿐겠지?      공포         NaN         NaN         NaN  분노   \n",
       "2            아직너무초기라서 그런거죠?      공포         NaN         NaN         NaN  슬픔   \n",
       "3             유치원버스 사고 낫다던데      공포         NaN         NaN         NaN  중립   \n",
       "4               근데 원래이런거맞나요      공포         NaN         NaN         NaN  행복   \n",
       "\n",
       "   5468.0  Label  \n",
       "0  5898.0      1  \n",
       "1  5665.0      1  \n",
       "2  5267.0      1  \n",
       "3  4830.0      1  \n",
       "4  6037.0      1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping_emotions = {\n",
    "  \"분노\" : 0,\n",
    "  \"혐오\" : 0,\n",
    "  \"중립\" : 1,\n",
    "  \"기쁨\" : 1,\n",
    "  \"행복\" : 1,\n",
    "  \"공포\" : 1,\n",
    "  \"불안\" : 1,\n",
    "  }\n",
    "\n",
    "dataset[\"Label\"] = dataset[\"Emotion\"].map(mapping_emotions)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0        0\n",
       "연령                0\n",
       "성별                0\n",
       "상황키워드             0\n",
       "신체질환              0\n",
       "감정_대분류            0\n",
       "감정_소분류            0\n",
       "사람문장1             0\n",
       "시스템문장1            0\n",
       "사람문장2             0\n",
       "시스템문장2            0\n",
       "사람문장3         10890\n",
       "시스템문장3        10890\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "emotion_data_1 = pd.read_excel(\"/mnt/c/Users/Admin/documents/kobert_project/감성대화말뭉치(최종데이터)_Training.xlsx\")\n",
    "emotion_data_2 = pd.read_excel(\"/mnt/c/Users/Admin/documents/kobert_project/감성대화말뭉치(최종데이터)_Validation.xlsx\")\n",
    "emotion_data = pd.concat([emotion_data_1, emotion_data_2], axis=0)\n",
    "emotion_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "불안    10433\n",
       "분노    10417\n",
       "상처    10150\n",
       "슬픔    10128\n",
       "당황     9804\n",
       "기쁨     7339\n",
       "Name: 감정_대분류, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_data.감정_대분류.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>사람문장1</th>\n",
       "      <th>사람문장2</th>\n",
       "      <th>사람문장3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6564</th>\n",
       "      <td>우리 아파트 동대표들이 작당하고 돈 장난을 친다니 혐오스러운 놈들이야.</td>\n",
       "      <td>술집에서 자기들끼리 싸우고 나서 한 놈이 약간 까발린 모양이야. 대한민국 아파트가 ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6565</th>\n",
       "      <td>나 오늘 회사에서 넘어져서 많이 부끄러웠어.</td>\n",
       "      <td>다친 곳은 전혀 없는데 사람 많은 곳에서 넘어져서 그냥 많이 부끄러웠어.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6573</th>\n",
       "      <td>오늘 애들이 갑자기 나만 놔두고 밥을 먹으러 가서 당황했어.</td>\n",
       "      <td>물어볼 용기가 안 나서 못 했어.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6580</th>\n",
       "      <td>여자친구랑 뷔페를 갔는데 좀 당황스러운 일이 있었어.</td>\n",
       "      <td>여자친구가 처음에 디저트를 접시에 마구 퍼오더라고. 근데 매우 많은 양이었어.</td>\n",
       "      <td>인증샷만 찍고 거의 먹지 않더라고. 음식을 함부로 하는 모습에 실망했어.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6583</th>\n",
       "      <td>부모님이 동생만 챙기셔서 너무 힘들어.</td>\n",
       "      <td>집에 들어가는 것이 너무 싫고 힘들어.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6584</th>\n",
       "      <td>가족과 사이가 좋지 않아서 내가 한심한 기분이 들어.</td>\n",
       "      <td>글쎄 말하고 싶지 않아.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6594</th>\n",
       "      <td>이게 갑자기 무슨 일이지?</td>\n",
       "      <td>친구에게서 갑자기 다시는 연락하지 말라는 문자를 받았어. 이런 통보는 또 처음이야.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6595</th>\n",
       "      <td>상사한테 한 소리 들었어. 당황스러워.</td>\n",
       "      <td>동료가 실수한 일을 나한테 얘기하시네!</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6599</th>\n",
       "      <td>성격이 내성적이라서 사람들 앞에 나서기가 부끄러워.</td>\n",
       "      <td>부끄러워서 많은 사람 앞에서 대화를 길게 못 하겠어. 도움이 되는 책을 사서 읽기 ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6600</th>\n",
       "      <td>회사 일로 발표를 할 땐 늘 상사에게 혼나게 돼.</td>\n",
       "      <td>사람들 앞에서 얘기하는 게 부끄러워서 늘 발표를 망쳐. 어떻게 해야 할지 모르겠어.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6612</th>\n",
       "      <td>요새 집에만 있고 친구를 못 만나니까 외로워.</td>\n",
       "      <td>얼굴 보고 이야기하면 참 좋을 텐데. 만날 수가 없으니 말이야.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6617</th>\n",
       "      <td>바빠지다 보니 부모님과의 관계에 소홀해진 것 같아서 죄책감이 들어.</td>\n",
       "      <td>응. 지금부터는 연락도 잘 드리고 자주 찾아뵈려고 해.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6618</th>\n",
       "      <td>오늘 의도치 않게 친구의 비밀을 말해버렸어.</td>\n",
       "      <td>친구에게 정말 미안하고 죄책감이 들었어.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6626</th>\n",
       "      <td>내가 상사가 업소에 다닌다는 소문을 들었어.</td>\n",
       "      <td>생각지도 못했는데 정말 혐오스러워. 앞으로 어떻게 대해야 할지 모르겠어.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6627</th>\n",
       "      <td>코로나 시국이 되니 일부 기독교도들이 혐오스럽게 느껴져.</td>\n",
       "      <td>코로나 시국에 광화문 집회니 모여서 예배를 드리니 왜 그러는 거야? 다 잡아갔으면 ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6636</th>\n",
       "      <td>나랑 비슷한 시기에 결혼하는 친구는 시댁에서 집을 해줘서 너무 부러워. 우리는 대출...</td>\n",
       "      <td>우리만 뒤처지는 것 같고. 그래도 열심히 살다 보면 우리도 집을 살 수 있겠지?</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6637</th>\n",
       "      <td>친구 한 명이 결혼해서 아이를 가졌는데 너무 행복해 보이더라. 기분이 좋지만은 않아.</td>\n",
       "      <td>나도 결혼했지만 아이도 생기지 않고 그 정도로 행복하지 않거든. 친구보다 못사는 것...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6638</th>\n",
       "      <td>남들은 결혼 전에 일억을 모았다는데 난 뭐를 한 것인지 모르겠어. 자괴감만 드네.</td>\n",
       "      <td>요새 집값이 너무 올라서 한숨만 나와.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6639</th>\n",
       "      <td>나보다 결혼을 먼저 한 친구가 부러워. 그 친구 남편은 직장도 내 남편보다 좋고 키...</td>\n",
       "      <td>맞아. 그 친구와 비교하게 되니 자존감이 낮아지는 기분이야.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6640</th>\n",
       "      <td>친구들 모두 결혼하고 나만 혼자 남아서 쓸쓸하네.</td>\n",
       "      <td>맞아. 하지만 그렇다고 아무나하고 결혼할 수도 없잖아.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  사람문장1  \\\n",
       "6564            우리 아파트 동대표들이 작당하고 돈 장난을 친다니 혐오스러운 놈들이야.   \n",
       "6565                           나 오늘 회사에서 넘어져서 많이 부끄러웠어.   \n",
       "6573                  오늘 애들이 갑자기 나만 놔두고 밥을 먹으러 가서 당황했어.   \n",
       "6580                      여자친구랑 뷔페를 갔는데 좀 당황스러운 일이 있었어.   \n",
       "6583                              부모님이 동생만 챙기셔서 너무 힘들어.   \n",
       "6584                      가족과 사이가 좋지 않아서 내가 한심한 기분이 들어.   \n",
       "6594                                     이게 갑자기 무슨 일이지?   \n",
       "6595                              상사한테 한 소리 들었어. 당황스러워.   \n",
       "6599                       성격이 내성적이라서 사람들 앞에 나서기가 부끄러워.   \n",
       "6600                        회사 일로 발표를 할 땐 늘 상사에게 혼나게 돼.   \n",
       "6612                          요새 집에만 있고 친구를 못 만나니까 외로워.   \n",
       "6617              바빠지다 보니 부모님과의 관계에 소홀해진 것 같아서 죄책감이 들어.   \n",
       "6618                           오늘 의도치 않게 친구의 비밀을 말해버렸어.   \n",
       "6626                           내가 상사가 업소에 다닌다는 소문을 들었어.   \n",
       "6627                    코로나 시국이 되니 일부 기독교도들이 혐오스럽게 느껴져.   \n",
       "6636  나랑 비슷한 시기에 결혼하는 친구는 시댁에서 집을 해줘서 너무 부러워. 우리는 대출...   \n",
       "6637    친구 한 명이 결혼해서 아이를 가졌는데 너무 행복해 보이더라. 기분이 좋지만은 않아.   \n",
       "6638      남들은 결혼 전에 일억을 모았다는데 난 뭐를 한 것인지 모르겠어. 자괴감만 드네.   \n",
       "6639  나보다 결혼을 먼저 한 친구가 부러워. 그 친구 남편은 직장도 내 남편보다 좋고 키...   \n",
       "6640                        친구들 모두 결혼하고 나만 혼자 남아서 쓸쓸하네.   \n",
       "\n",
       "                                                  사람문장2  \\\n",
       "6564  술집에서 자기들끼리 싸우고 나서 한 놈이 약간 까발린 모양이야. 대한민국 아파트가 ...   \n",
       "6565           다친 곳은 전혀 없는데 사람 많은 곳에서 넘어져서 그냥 많이 부끄러웠어.   \n",
       "6573                                 물어볼 용기가 안 나서 못 했어.   \n",
       "6580        여자친구가 처음에 디저트를 접시에 마구 퍼오더라고. 근데 매우 많은 양이었어.   \n",
       "6583                              집에 들어가는 것이 너무 싫고 힘들어.   \n",
       "6584                                      글쎄 말하고 싶지 않아.   \n",
       "6594     친구에게서 갑자기 다시는 연락하지 말라는 문자를 받았어. 이런 통보는 또 처음이야.   \n",
       "6595                              동료가 실수한 일을 나한테 얘기하시네!   \n",
       "6599  부끄러워서 많은 사람 앞에서 대화를 길게 못 하겠어. 도움이 되는 책을 사서 읽기 ...   \n",
       "6600     사람들 앞에서 얘기하는 게 부끄러워서 늘 발표를 망쳐. 어떻게 해야 할지 모르겠어.   \n",
       "6612                얼굴 보고 이야기하면 참 좋을 텐데. 만날 수가 없으니 말이야.   \n",
       "6617                     응. 지금부터는 연락도 잘 드리고 자주 찾아뵈려고 해.   \n",
       "6618                             친구에게 정말 미안하고 죄책감이 들었어.   \n",
       "6626           생각지도 못했는데 정말 혐오스러워. 앞으로 어떻게 대해야 할지 모르겠어.   \n",
       "6627  코로나 시국에 광화문 집회니 모여서 예배를 드리니 왜 그러는 거야? 다 잡아갔으면 ...   \n",
       "6636       우리만 뒤처지는 것 같고. 그래도 열심히 살다 보면 우리도 집을 살 수 있겠지?   \n",
       "6637  나도 결혼했지만 아이도 생기지 않고 그 정도로 행복하지 않거든. 친구보다 못사는 것...   \n",
       "6638                              요새 집값이 너무 올라서 한숨만 나와.   \n",
       "6639                  맞아. 그 친구와 비교하게 되니 자존감이 낮아지는 기분이야.   \n",
       "6640                     맞아. 하지만 그렇다고 아무나하고 결혼할 수도 없잖아.   \n",
       "\n",
       "                                         사람문장3  \n",
       "6564                                       NaN  \n",
       "6565                                       NaN  \n",
       "6573                                       NaN  \n",
       "6580  인증샷만 찍고 거의 먹지 않더라고. 음식을 함부로 하는 모습에 실망했어.  \n",
       "6583                                       NaN  \n",
       "6584                                       NaN  \n",
       "6594                                       NaN  \n",
       "6595                                       NaN  \n",
       "6599                                       NaN  \n",
       "6600                                       NaN  \n",
       "6612                                       NaN  \n",
       "6617                                       NaN  \n",
       "6618                                       NaN  \n",
       "6626                                       NaN  \n",
       "6627                                       NaN  \n",
       "6636                                       NaN  \n",
       "6637                                       NaN  \n",
       "6638                                       NaN  \n",
       "6639                                       NaN  \n",
       "6640                                       NaN  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_data.loc[emotion_data.감정_대분류 == '당황',['사람문장1','사람문장2','사람문장3']].tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가공 전 데이터 개수 : 58271\n",
      "가공 후 데이터 개수 : 22514\n",
      "가공 후 null값 개수 \n",
      "감정_대분류    0\n",
      "사람문장1     0\n",
      "사람문장2     0\n",
      "사람문장3     0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Sent1</th>\n",
       "      <th>Sent2</th>\n",
       "      <th>Sent3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>기쁨</td>\n",
       "      <td>아프긴 해도 혼자 사니까 다른 사람 신경 쓰지 않아도 되고 모은 돈으로 살 수 있어...</td>\n",
       "      <td>혼자 살아서 돈을 넉넉하게 모을 수 있었어. 병원비가 꽤 나가더라고.</td>\n",
       "      <td>내가 좋아하는 취미 생활을 찾는 것이 가장 중요한 것 같아.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>기쁨</td>\n",
       "      <td>경제적으로 여유 있어 아픈 것만 빼면 삶이 너무 즐거워.</td>\n",
       "      <td>어릴 때 열심히 일해서 벌어 두길 정말 잘한 것 같아.</td>\n",
       "      <td>건강을 위해 운동을 꾸준히 하며 관리하고 있어.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>기쁨</td>\n",
       "      <td>건강 검진을 받아도 경제적으로 여유로워 마음이 편해.</td>\n",
       "      <td>맞아. 검사비가 많이 나오니까 전에는 그런 점들이 걱정이었어.</td>\n",
       "      <td>재테크에 성공해서 경제적인 여유가 생겼어.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>기쁨</td>\n",
       "      <td>만성 질환을 앓고 있어도 경제적으로 여유로워 약값 걱정이 안 돼.</td>\n",
       "      <td>그래서 약을 꾸준히 먹고 있어. 만성 질환엔 꾸준히 약을 먹는 게 중요하더라고.</td>\n",
       "      <td>당연하지. 계속 잘 관리해서 건강하게 오래오래 살 거야.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>기쁨</td>\n",
       "      <td>아들이 병원 갈 때 함께 가주고 병원비도 내줘서 너무 든든해.</td>\n",
       "      <td>아들이 병원 예약이 있는 날마다 항상 함께 가 줘. 병원비도 부담될 텐데 전혀 내색...</td>\n",
       "      <td>아들을 위해서 더욱 건강하게 관리해야지.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22509</th>\n",
       "      <td>기쁨</td>\n",
       "      <td>출산을 무사히 마치고 퇴원했어. 많은 사람이 축하해줬는데 그분들께 너무 감사해.</td>\n",
       "      <td>출산 직후에는 정신이 없었는데 퇴원하고 여유가 생기니 주변 사람들에게 감사하는 마음...</td>\n",
       "      <td>그렇지. 이제 힘내서 사람들에게 감사 인사를 전하는 연락을 할 거야.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22510</th>\n",
       "      <td>분노</td>\n",
       "      <td>나 정말 화가 나 죽겠어.</td>\n",
       "      <td>친구가 약속 시각에 자꾸 늦어.</td>\n",
       "      <td>말해봐도 소용이 없어.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22511</th>\n",
       "      <td>기쁨</td>\n",
       "      <td>내 나이가 되면 원망도 줄어들고 고마운 마음이 더 많아지는 거 같아.</td>\n",
       "      <td>키워주신 부모님 묵묵히 가정을 지켜준 남편에게 고마워.</td>\n",
       "      <td>부모님께는 자주 찾아뵙는 게 우선일 거고 남편에겐 힘낼 수 있는 말을 자주 해 줄 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22512</th>\n",
       "      <td>불안</td>\n",
       "      <td>나 요즘 친구들이 나를 은근히 피하고 장난도 더 심하게 치는 것 같아서 혹시 나를 ...</td>\n",
       "      <td>친구들이 나를 왜 피하는지도 모르겠어.</td>\n",
       "      <td>좋아하는 노래를 한 곡 듣고 기분이 나아진 상태에서 친구들과 대화를 하면 좋을 것 같아.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22513</th>\n",
       "      <td>기쁨</td>\n",
       "      <td>나 요즘 마음이 편안해.</td>\n",
       "      <td>학교에서 따돌림당하는 친구를 도와줬거든.</td>\n",
       "      <td>함께 점심도 먹고 친하게 지내고 있어.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22514 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Emotion                                              Sent1  \\\n",
       "0          기쁨  아프긴 해도 혼자 사니까 다른 사람 신경 쓰지 않아도 되고 모은 돈으로 살 수 있어...   \n",
       "1          기쁨                    경제적으로 여유 있어 아픈 것만 빼면 삶이 너무 즐거워.   \n",
       "2          기쁨                      건강 검진을 받아도 경제적으로 여유로워 마음이 편해.   \n",
       "3          기쁨               만성 질환을 앓고 있어도 경제적으로 여유로워 약값 걱정이 안 돼.   \n",
       "4          기쁨                 아들이 병원 갈 때 함께 가주고 병원비도 내줘서 너무 든든해.   \n",
       "...       ...                                                ...   \n",
       "22509      기쁨       출산을 무사히 마치고 퇴원했어. 많은 사람이 축하해줬는데 그분들께 너무 감사해.   \n",
       "22510      분노                                     나 정말 화가 나 죽겠어.   \n",
       "22511      기쁨             내 나이가 되면 원망도 줄어들고 고마운 마음이 더 많아지는 거 같아.   \n",
       "22512      불안  나 요즘 친구들이 나를 은근히 피하고 장난도 더 심하게 치는 것 같아서 혹시 나를 ...   \n",
       "22513      기쁨                                      나 요즘 마음이 편안해.   \n",
       "\n",
       "                                                   Sent2  \\\n",
       "0                 혼자 살아서 돈을 넉넉하게 모을 수 있었어. 병원비가 꽤 나가더라고.   \n",
       "1                         어릴 때 열심히 일해서 벌어 두길 정말 잘한 것 같아.   \n",
       "2                     맞아. 검사비가 많이 나오니까 전에는 그런 점들이 걱정이었어.   \n",
       "3           그래서 약을 꾸준히 먹고 있어. 만성 질환엔 꾸준히 약을 먹는 게 중요하더라고.   \n",
       "4      아들이 병원 예약이 있는 날마다 항상 함께 가 줘. 병원비도 부담될 텐데 전혀 내색...   \n",
       "...                                                  ...   \n",
       "22509  출산 직후에는 정신이 없었는데 퇴원하고 여유가 생기니 주변 사람들에게 감사하는 마음...   \n",
       "22510                                  친구가 약속 시각에 자꾸 늦어.   \n",
       "22511                     키워주신 부모님 묵묵히 가정을 지켜준 남편에게 고마워.   \n",
       "22512                              친구들이 나를 왜 피하는지도 모르겠어.   \n",
       "22513                             학교에서 따돌림당하는 친구를 도와줬거든.   \n",
       "\n",
       "                                                   Sent3  \n",
       "0                      내가 좋아하는 취미 생활을 찾는 것이 가장 중요한 것 같아.  \n",
       "1                             건강을 위해 운동을 꾸준히 하며 관리하고 있어.  \n",
       "2                                재테크에 성공해서 경제적인 여유가 생겼어.  \n",
       "3                        당연하지. 계속 잘 관리해서 건강하게 오래오래 살 거야.  \n",
       "4                                 아들을 위해서 더욱 건강하게 관리해야지.  \n",
       "...                                                  ...  \n",
       "22509             그렇지. 이제 힘내서 사람들에게 감사 인사를 전하는 연락을 할 거야.  \n",
       "22510                                       말해봐도 소용이 없어.  \n",
       "22511  부모님께는 자주 찾아뵙는 게 우선일 거고 남편에겐 힘낼 수 있는 말을 자주 해 줄 ...  \n",
       "22512  좋아하는 노래를 한 곡 듣고 기분이 나아진 상태에서 친구들과 대화를 하면 좋을 것 같아.  \n",
       "22513                              함께 점심도 먹고 친하게 지내고 있어.  \n",
       "\n",
       "[22514 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_column = ['감정_대분류', '사람문장1', '사람문장2', '사람문장3']\n",
    "print(f'가공 전 데이터 개수 : {len(emotion_data)}')\n",
    "emotion_data = emotion_data.loc[emotion_data.감정_대분류 != '슬픔', target_column]\n",
    "emotion_data = emotion_data.loc[emotion_data.감정_대분류 != '상처', target_column]\n",
    "emotion_data = emotion_data.loc[emotion_data.감정_대분류 != '당황', target_column]\n",
    "\n",
    "\n",
    "emotion_data_mod = emotion_data.dropna(axis=0).reset_index(drop=True)\n",
    "print(f'가공 후 데이터 개수 : {len(emotion_data_mod)}')\n",
    "print(f'가공 후 null값 개수 \\n{emotion_data_mod.isnull().sum()}')\n",
    "\n",
    "\n",
    "emotion_data_mod.columns = ['Emotion', 'Sent1', 'Sent2', 'Sent3']\n",
    "emotion_data_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "분노    8576\n",
       "불안    8454\n",
       "기쁨    5484\n",
       "Name: Emotion, dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_data_mod.Emotion.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_emotions = {\n",
    "  \"분노\" : 0,\n",
    "  \"혐오\" : 0,\n",
    "  \"중립\" : 1,\n",
    "  \"기쁨\" : 1,\n",
    "  \"행복\" : 1,\n",
    "  \"공포\" : 1,\n",
    "  \"불안\" : 1,\n",
    "  }\n",
    "emotion_data_mod[\"Label\"] = emotion_data_mod[\"Emotion\"].map(mapping_emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "변환 정보의 길이 : 67542\n",
      "원본 데이터의 길이 : 22514\n",
      "데이터 변환이 완료되었습니다.\n"
     ]
    }
   ],
   "source": [
    "data_list_one = []\n",
    "for i in range(1,4):\n",
    "  for q, label in zip(emotion_data_mod[f'Sent{i}'], emotion_data_mod['Label'])  :\n",
    "    data = []\n",
    "    data.append(q)\n",
    "    data.append(str(label))\n",
    "\n",
    "    data_list_one.append(data)\n",
    "\n",
    "print(f\"변환 정보의 길이 : {len(data_list_one)}\\n원본 데이터의 길이 : {len(emotion_data_mod)}\")\n",
    "if len(data_list_one) == len(emotion_data_mod)*3:\n",
    "  print(\"데이터 변환이 완료되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list_two = []\n",
    "for q, label in zip(dataset['Sentence'], dataset['Label'])  :\n",
    "    data = []\n",
    "    data.append(q)\n",
    "    data.append(str(label))\n",
    "\n",
    "    data_list_two.append(data)\n",
    "\n",
    "data_list = data_list_one + data_list_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71228\n",
      "23743\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data =  train_test_split(data_list, test_size=0.25, random_state=0)\n",
    "print(len(train_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
    "                 pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "\n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting parameters\n",
    "max_len = 64\n",
    "batch_size = 32\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 10\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate =  5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model. /mnt/c/Users/Admin/Documents/kobert_project/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
     ]
    }
   ],
   "source": [
    "#토큰화\n",
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
    "\n",
    "data_train = BERTDataset(train_data, 0, 1, tok, max_len, True, False)\n",
    "data_test = BERTDataset(test_data, 0, 1, tok, max_len, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)\n",
    "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=2,   ##클래스 수 조정##\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_27160/1794942048.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#BERT 모델 불러오기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBERTClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbertmodel\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mdr_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#optimizer와 schedule 설정\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mno_decay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'bias'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'LayerNorm.weight'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kobert/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    897\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m~/miniconda3/envs/kobert/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kobert/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kobert/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kobert/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kobert/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    895\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    896\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 897\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#BERT 모델 불러오기\n",
    "model = BERTClassifier(bertmodel,  dr_rate=0.5).to(device)\n",
    "\n",
    "#optimizer와 schedule 설정\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n",
    "\n",
    "#정확도 측정을 위한 함수 정의\n",
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc\n",
    "    \n",
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryud904/miniconda3/envs/kobert/lib/python3.7/site-packages/ipykernel_launcher.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd2fadd1d5fc4e14a2a1f72ae9fd1c8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2226 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1 loss 0.7449871301651001 train acc 0.4375\n",
      "epoch 1 batch id 201 loss 0.7316056489944458 train acc 0.5309390547263682\n",
      "epoch 1 batch id 401 loss 0.4899197816848755 train acc 0.5912562344139651\n",
      "epoch 1 batch id 601 loss 0.5494669079780579 train acc 0.6452787021630616\n",
      "epoch 1 batch id 801 loss 0.39243295788764954 train acc 0.670099875156055\n",
      "epoch 1 batch id 1001 loss 0.2891656160354614 train acc 0.6878434065934066\n",
      "epoch 1 batch id 1201 loss 0.2626956105232239 train acc 0.6985064529558701\n",
      "epoch 1 batch id 1401 loss 0.4477037191390991 train acc 0.707151142041399\n",
      "epoch 1 batch id 1601 loss 0.3023909330368042 train acc 0.7143386945658963\n",
      "epoch 1 batch id 1801 loss 0.49119502305984497 train acc 0.7186979455857857\n",
      "epoch 1 batch id 2001 loss 0.44536805152893066 train acc 0.7223419540229885\n",
      "epoch 1 batch id 2201 loss 0.5472137331962585 train acc 0.7253947069513857\n",
      "epoch 1 train acc 0.7258254716981132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryud904/miniconda3/envs/kobert/lib/python3.7/site-packages/ipykernel_launcher.py:23: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64ee495ebc22448280c8f2b0d348d176",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/742 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 test acc 0.7485191505086515\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bef7b779e4894588a5c7af8ecb6ba0cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2226 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 batch id 1 loss 0.5363180637359619 train acc 0.65625\n",
      "epoch 2 batch id 201 loss 0.3737381398677826 train acc 0.7510883084577115\n",
      "epoch 2 batch id 401 loss 0.45681169629096985 train acc 0.7602088528678305\n",
      "epoch 2 batch id 601 loss 0.4725058972835541 train acc 0.7676268718801996\n",
      "epoch 2 batch id 801 loss 0.3120129108428955 train acc 0.7705992509363296\n",
      "epoch 2 batch id 1001 loss 0.2274787276983261 train acc 0.7734140859140859\n",
      "epoch 2 batch id 1201 loss 0.21613189578056335 train acc 0.7757077435470441\n",
      "epoch 2 batch id 1401 loss 0.29484638571739197 train acc 0.7784172019985724\n",
      "epoch 2 batch id 1601 loss 0.2899538278579712 train acc 0.7808986570893192\n",
      "epoch 2 batch id 1801 loss 0.3856222331523895 train acc 0.782638117712382\n",
      "epoch 2 batch id 2001 loss 0.3717832863330841 train acc 0.7844202898550725\n",
      "epoch 2 batch id 2201 loss 0.37108850479125977 train acc 0.7873267832803271\n",
      "epoch 2 train acc 0.7876255455012193\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7990390182fb447ba27f012ddf2b2e9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/742 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 test acc 0.7745888944439614\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35663ad75b3343fb90b5cb8dfc4fa843",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2226 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 batch id 1 loss 0.33633384108543396 train acc 0.875\n",
      "epoch 3 batch id 201 loss 0.3185182809829712 train acc 0.7997512437810945\n",
      "epoch 3 batch id 401 loss 0.26353883743286133 train acc 0.8116427680798005\n",
      "epoch 3 batch id 601 loss 0.40164297819137573 train acc 0.8183236272878536\n",
      "epoch 3 batch id 801 loss 0.25496798753738403 train acc 0.8202637328339576\n",
      "epoch 3 batch id 1001 loss 0.1574937254190445 train acc 0.8241446053946054\n",
      "epoch 3 batch id 1201 loss 0.16149351000785828 train acc 0.8257701915070774\n",
      "epoch 3 batch id 1401 loss 0.16535460948944092 train acc 0.8289168451106352\n",
      "epoch 3 batch id 1601 loss 0.22305983304977417 train acc 0.8303013741411618\n",
      "epoch 3 batch id 1801 loss 0.32953664660453796 train acc 0.8316560244308717\n",
      "epoch 3 batch id 2001 loss 0.20619897544384003 train acc 0.8333801849075462\n",
      "epoch 3 batch id 2201 loss 0.3261304795742035 train acc 0.8361682189913675\n",
      "epoch 3 train acc 0.8363716949043769\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95f8d5c3074842c19460806f32f812ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/742 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 test acc 0.7742098513172768\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eedc053a91894b1490ec90b0c2359f7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2226 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 batch id 1 loss 0.3528282344341278 train acc 0.90625\n",
      "epoch 4 batch id 201 loss 0.26140502095222473 train acc 0.8597636815920398\n",
      "epoch 4 batch id 401 loss 0.48115333914756775 train acc 0.8641677057356608\n",
      "epoch 4 batch id 601 loss 0.33411723375320435 train acc 0.8692283693843594\n",
      "epoch 4 batch id 801 loss 0.24317553639411926 train acc 0.8718398876404494\n",
      "epoch 4 batch id 1001 loss 0.0741073414683342 train acc 0.8725337162837162\n",
      "epoch 4 batch id 1201 loss 0.05176602676510811 train acc 0.8743755203996669\n",
      "epoch 4 batch id 1401 loss 0.07167339324951172 train acc 0.8779220199857245\n",
      "epoch 4 batch id 1601 loss 0.10722463577985764 train acc 0.8789428482198626\n",
      "epoch 4 batch id 1801 loss 0.30219903588294983 train acc 0.8805698223209328\n",
      "epoch 4 batch id 2001 loss 0.20641902089118958 train acc 0.8816373063468266\n",
      "epoch 4 batch id 2201 loss 0.37706759572029114 train acc 0.8832206951385734\n",
      "epoch 4 train acc 0.8834312026697472\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12216403821f45bda467c5125d9b3993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/742 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 test acc 0.7736990261716373\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ed2ffc3cd67400c85bd918fd2e7b015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2226 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 batch id 1 loss 0.2936319410800934 train acc 0.875\n",
      "epoch 5 batch id 201 loss 0.18882468342781067 train acc 0.8970771144278606\n",
      "epoch 5 batch id 401 loss 0.30801352858543396 train acc 0.9006390274314214\n",
      "epoch 5 batch id 601 loss 0.2000262588262558 train acc 0.9057300332778702\n",
      "epoch 5 batch id 801 loss 0.09665460139513016 train acc 0.9079275905118602\n",
      "epoch 5 batch id 1001 loss 0.05116917937994003 train acc 0.9105894105894106\n",
      "epoch 5 batch id 1201 loss 0.026855556294322014 train acc 0.9127289758534555\n",
      "epoch 5 batch id 1401 loss 0.0471235066652298 train acc 0.9153283369022127\n",
      "epoch 5 batch id 1601 loss 0.14082352817058563 train acc 0.9155605871330419\n",
      "epoch 5 batch id 1801 loss 0.24193720519542694 train acc 0.9167649916712938\n",
      "epoch 5 batch id 2001 loss 0.189158633351326 train acc 0.9168228385807097\n",
      "epoch 5 batch id 2201 loss 0.3069063425064087 train acc 0.918488755111313\n",
      "epoch 5 train acc 0.918497705686048\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c50dc72f4f84978936fc16211a70b4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/742 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 test acc 0.7771973632727589\n"
     ]
    }
   ],
   "source": [
    "for e in range(num_epochs//2):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length = valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "    \n",
    "    model.eval()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        test_acc += calc_accuracy(out, label)\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))\n",
    "    \n",
    "    if (e+1) % 2 == 1:\n",
    "        torch.save({'epochs' : num_epochs+1,\n",
    "                    'model' : model.state_dict(),\n",
    "                    'optimizer' : optimizer.state_dict()\n",
    "                    }, f'/mnt/c/Users/Admin/documents/kobert_project/checkpoint/checkpoint_ep_{e+1}_{train_acc/(batch_id+1)}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습 재개(gpu 메모리로 인해 나눠서 학습을 진행합니다.)\n",
    "\n",
    "checkpoint = torch.load('./checkpoint/checkpoint_ep_4_2.650293608009241.pt')\n",
    "model = BERTClassifier(bertmodel,  dr_rate=0.5)\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for e in range(num_epochs//2, num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length = valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "    \n",
    "    model.eval()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        test_acc += calc_accuracy(out, label)\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))\n",
    "    \n",
    "    if (e+1) % 2 == 0:\n",
    "        torch.save({'epochs' : num_epochs+1,\n",
    "                    'model' : model.state_dict(),\n",
    "                    'optimizer' : optimizer.state_dict()\n",
    "                    }, f'/mnt/c/Users/Admin/documents/kobert_project/checkpoint/checkpoint_ep_{e+1}_{train_acc/(batch_id+1)}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model. /mnt/c/Users/Admin/Documents/kobert_project/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
     ]
    }
   ],
   "source": [
    "#토큰화\n",
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
    "\n",
    "def predict(predict_sentence):\n",
    "\n",
    "    data = [predict_sentence, '0']\n",
    "    dataset_another = [data]\n",
    "\n",
    "    another_test = BERTDataset(dataset_another, 0, 1, tok, max_len, True, False)\n",
    "    test_dataloader = torch.utils.data.DataLoader(another_test, batch_size=batch_size, num_workers=5)\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "\n",
    "\n",
    "        test_eval=[]\n",
    "        for i in out:\n",
    "            logits=i\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "\n",
    "            if np.argmax(logits) == 0:\n",
    "                test_eval.append(\"분노 및 혐오가\")\n",
    "            elif np.argmax(logits) == 1:\n",
    "                test_eval.append(\"일반적 감정이\")\n",
    "            \n",
    "        print(\">> 입력하신 내용에서 \" + test_eval[0] + \" 감지됩니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사랑해\n",
      ">> 입력하신 내용에서 일반적 감정이 감지됩니다.\n",
      "\n",
      "\n",
      "아 그게 아니잖아....\n",
      ">> 입력하신 내용에서 일반적 감정이 감지됩니다.\n",
      "\n",
      "\n",
      "아 그게 아니라;\n",
      ">> 입력하신 내용에서 일반적 감정이 감지됩니다.\n",
      "\n",
      "\n",
      "아니아니 그게 아니라\n",
      ">> 입력하신 내용에서 분노 및 혐오가 감지됩니다.\n",
      "\n",
      "\n",
      "짜증나....\n",
      ">> 입력하신 내용에서 분노 및 혐오가 감지됩니다.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#질문 무한반복하기! 0 입력시 종료\n",
    "\n",
    "for i in range(5) :\n",
    "  sentence = input(\"하고싶은 말을 입력해주세요 : \")\n",
    "  print(sentence)\n",
    "  predict(sentence)\n",
    "  print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kobert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
